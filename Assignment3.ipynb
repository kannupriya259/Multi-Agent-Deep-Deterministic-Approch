{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dWnBGMp95sK"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysLT4oxq8c6L"
      },
      "outputs": [],
      "source": [
        "!pip install supersuit\n",
        "!pip install stable-baselines3\n",
        "!pip install pettingzoo==1.21.0\n",
        "!pip install git+https://github.com/Markus28/tianshou@support_pz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1l9dmqQTWlh"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.ppo import CnnPolicy\n",
        "from stable_baselines3 import PPO\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "import supersuit as ss\n",
        "import pettingzoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfQOl52I8tk4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import copy\n",
        "from collections import namedtuple, deque\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from supersuit import color_reduction_v0, frame_stack_v1, resize_v1\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "from tianshou.env.pettingzoo_env import PettingZooEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiF3wsSu998x"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EveEEDOjD5lF"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "BUFFER_SIZE = int(1e6)\n",
        "BATCH_SIZE = 512\n",
        "GAMMA = 0.95\n",
        "TAU = 0.05\n",
        "LR_ACTOR = 1e-3\n",
        "LR_CRITIC = 1e-3\n",
        "WEIGHT_DECAY = 0\n",
        "UPDATE_EVERY = 1\n",
        "NO_UPDATES = 1\n",
        "\n",
        "EPSILON = 1e-5\n",
        "ALPHA = 0.6\n",
        "TOTAL_EPISODES=10000\n",
        "NOISE_SCALE = 1\n",
        "NOISE_DECAY_LIMIT=1200\n",
        "BETA_EPISODES_LIMIT = 2000\n",
        "\n",
        "GREEDY_EPSILON = 0.01\n",
        "GREEDY_EPSILON_MIN = 0.01  \n",
        "GREEDY_EPSILON_DECAY= 0.0005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaPqJIgi-At-"
      },
      "source": [
        "### Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLhxcgzN89R2"
      },
      "outputs": [],
      "source": [
        "stack_size = 4\n",
        "frame_size = (64, 64)\n",
        "max_cycles = 125\n",
        "total_episodes = 5\n",
        "\n",
        "\"\"\" ENV SETUP \"\"\"\n",
        "env = pistonball_v6.parallel_env( continuous=False, max_cycles=max_cycles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLzld5ivHIqu"
      },
      "outputs": [],
      "source": [
        "env = color_reduction_v0(env)\n",
        "env = resize_v1(env, frame_size[0], frame_size[1])\n",
        "env = frame_stack_v1(env, stack_size=stack_size)\n",
        "num_agents = len(env.possible_agents)\n",
        "num_actions = env.action_space(env.possible_agents[0]).n\n",
        "observation_size = env.observation_space(env.possible_agents[0]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eNHu7f-HAm"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dck1ht7C8mg"
      },
      "outputs": [],
      "source": [
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor Model - Used to update policy.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
        "        super(Actor, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size*2, fc1_units)        \n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)        \n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)        \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)        \n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Actor (policy) network that maps states to the actions\"\"\"        \n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return torch.tanh(self.fc3(x))\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic Model to evaluate Value\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fcs1_units=64, fc2_units=64):\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fcs1 = nn.Linear(state_size*2, fcs1_units)            \n",
        "        self.fc2 = nn.Linear(fcs1_units+ (action_size*2), fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, 1)                \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)       \n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"critic/value network that maps (state, action) pairs to the Q-values\"\"\"\n",
        "        xs = F.leaky_relu(self.fcs1(state))\n",
        "        x = torch.cat((xs, action), dim=1)\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Hrok3-jDFBl"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size,  random_seed):        \n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size        \n",
        "        self.seed = random.seed(random_seed)\n",
        "        self.batch_size = BATCH_SIZE   \n",
        "        self.t_step = 0\n",
        "        self.seed = random.seed(random_seed)\n",
        "        self.decay_step = 0\n",
        "\n",
        "        # Actor Network (w/ Target Network)\n",
        "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
        "        \n",
        "        self.memory =  ReplayBuffer(BATCH_SIZE, BUFFER_SIZE, random_seed)\n",
        "        \n",
        "        # Noise process\n",
        "        self.noise = OUNoise( action_size , random_seed)\n",
        "       \n",
        "      \n",
        "    def step(self, state, action, reward, next_state, done,agent_number, beta):\n",
        "        \"\"\"Save experience in replay memory and use random sample to buffer from them.\"\"\"\n",
        "        # Save experience / reward                                       \n",
        "        self.memory.add(state, action, reward, next_state, done)            \n",
        "       \n",
        "        self.t_step +=1\n",
        "        \n",
        "        if self.t_step %UPDATE_EVERY == 0:\n",
        "            if self.memory.is_filled():                \n",
        "                self.learn(agent_number, GAMMA,beta) \n",
        "\n",
        "    def numpy_to_torch(self,data):\n",
        "        return torch.from_numpy(data).float().to(device)\n",
        "                                \n",
        "    def act(self, state, episode_num, add_noise=True ):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.from_numpy(state).float().to(device)                  \n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():              \n",
        "            action = self.actor_local(state).cpu().data.numpy()                                            \n",
        "        self.actor_local.train()               \n",
        "        if add_noise:                   \n",
        "            action = self.add_random_noise(action,episode_num)       \n",
        "        return np.clip(action, -1, 1)\n",
        "    \n",
        "    def noise_decay_schedule(self,episode_num):  \n",
        "        return max(0.0, NOISE_SCALE * (1 - (episode_num / NOISE_DECAY_LIMIT)))\n",
        "\n",
        "    def add_random_noise(self,action,episode_num):  \n",
        "        if episode_num < 500:\n",
        "            return np.random.randn(1,self.action_size)\n",
        "        action +=  self.noise_decay_schedule(episode_num) * self.noise.sample()\n",
        "        return action\n",
        "    \n",
        "    def reset(self):        \n",
        "        self.noise.reset()\n",
        "    \n",
        "    def learn(self, agent_number, gamma,beta):\n",
        "        \"\"\"Update policy and value parameters using given batch of experience tuples\"\"\"\n",
        "                                                  \n",
        "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
        "        # update critic, Get predicted next-state actions and Q values from target models\n",
        "                            \n",
        "        actions_next = self.actor_target(next_states)\n",
        "        if agent_number == 0:                     \n",
        "            actions_next = torch.cat((actions_next, actions[:,2:]), dim=1)\n",
        "        else:\n",
        "            actions_next = torch.cat((actions[:,:2], actions_next), dim=1)\n",
        "            \n",
        "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
        "        Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
        "        # Compute critic loss\n",
        "        Q_expected = self.critic_local(states, actions)                        \n",
        "        # Minimize the loss         \n",
        "        critic_loss =  F.mse_loss(Q_expected, Q_targets)\n",
        "        \n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        # update actor, Compute actor loss                 \n",
        "        actions_pred = self.actor_local(states)\n",
        "        if agent_number == 0:                    \n",
        "            actions_pred = torch.cat((actions_pred, actions[:,2:]), dim=1)\n",
        "        else:            \n",
        "            #actions_pred = self.actor_local(states[:,self.state_size:])\n",
        "            actions_pred = torch.cat((actions[:,:2], actions_pred), dim=1)\n",
        "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
        "        \n",
        "        # Minimize the loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # update target network#        \n",
        "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
        "        self.soft_update(self.actor_local, self.actor_target, TAU) \n",
        "                           \n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters\"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "\n",
        "class OUNoise:\n",
        "    def __init__(self, size, seed, mu=0., theta=0.13, sigma=0.2):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.seed = random.seed(seed)\n",
        "        self.size = size\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Update internal state\"\"\"\n",
        "        x = self.state        \n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
        "        self.state = x + dx\n",
        "        return self.state\n",
        "\n",
        "    \n",
        "class ReplayBuffer:    \n",
        "\n",
        "    def __init__(self, batch_size, buffer_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\"\"\"\n",
        "        \n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def is_filled(self):\n",
        "        \"\"\"Return the current size\"\"\"\n",
        "        return len(self.memory) >= self.batch_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7v9VsM1-K8H"
      },
      "source": [
        "### Multi Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts5ouduXEPzZ"
      },
      "outputs": [],
      "source": [
        "class MADDPGAgent():\n",
        "    def __init__(self,state_size,action_size,num_agents,random_seed):        \n",
        "        self.agents = [Agent(state_size, action_size,  random_seed) for _ in range(num_agents)]\n",
        "        self.num_agents = num_agents\n",
        "        self.total_state_size = state_size*num_agents \n",
        "        self.total_action_size = action_size * num_agents\n",
        "        self.t_step = 0\n",
        "    \n",
        "    def reset(self):\n",
        "        for agent in self.agents:\n",
        "            agent.reset()\n",
        "    \n",
        "    def act(self,states,episode_num):        \n",
        "        all_states = np.reshape(states, (1,self.total_state_size))     \n",
        "        actions = [self.agents[agent_num].act(all_states, episode_num) for agent_num in range(self.num_agents)] \n",
        "        actions = np.reshape(actions, (1,self.total_action_size))          \n",
        "        return actions\n",
        "        \n",
        "\n",
        "    def step(self,states,actions,rewards,next_states,dones,beta):   \n",
        "        states = np.reshape(states, (1,self.total_state_size))\n",
        "        actions = np.reshape(actions, (1,self.total_action_size))\n",
        "        next_states = np.reshape(next_states,(1,self.total_state_size))\n",
        "        for agent_num in range(self.num_agents):\n",
        "            self.agents[agent_num].step(states,actions,rewards[agent_num],next_states,dones[agent_num],agent_num,beta)        \n",
        "                              \n",
        "    def save_checkpt(self):\n",
        "        torch.save(self.agents[0].actor_local.state_dict(), 'checkpoint_actor1_ddpgv.pth')\n",
        "        torch.save(self.agents[0].critic_local.state_dict(), 'checkpoint_critic1_ddpgv.pth')\n",
        "        torch.save(self.agents[1].actor_local.state_dict(), 'checkpoint_actor2_ddpgv.pth')\n",
        "        torch.save(self.agents[1].critic_local.state_dict(), 'checkpoint_critic2_ddpgv.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggWxRSIHvmp",
        "outputId": "f14c99ec-ef85-4172-8e6b-55bc23b7d503"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_size = 180\n",
        "action_size = 3\n",
        "num_agents = env.unwrapped.num_agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV-mahO2ET2I"
      },
      "outputs": [],
      "source": [
        "agent = MADDPGAgent(state_size, action_size,num_agents,  0) \n",
        "\n",
        "def scale_beta(episode_num):\n",
        "    return min(1.0, ( episode_num / BETA_EPISODES_LIMIT))                            \n",
        "    \n",
        "def noise_decay_schedule(episode_num):  \n",
        "    return max(0.0, NOISE_SCALE * (1 - (episode_num / NOISE_DECAY_LIMIT)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itD6XJ20G6s2"
      },
      "outputs": [],
      "source": [
        "def maddpg(n_episodes=TOTAL_EPISODES, max_t=100, print_every=200):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    all_scores = []\n",
        "    rolling_avg_100 = []    \n",
        "    total_state_size = state_size*num_agents \n",
        "    total_action_size = action_size * num_agents\n",
        "    \n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        beta = scale_beta(i_episode)\n",
        "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
        "        states = env_info.vector_observations        \n",
        "        agent.reset()\n",
        "        scores = np.zeros(num_agents)\n",
        "        \n",
        "        while True: \n",
        "            actions = agent.act(states,i_episode)              \n",
        "            env_info = env.step(actions)[brain_name]\n",
        "            next_states = env_info.vector_observations      \n",
        "            rewards = env_info.rewards\n",
        "            dones = env_info.local_done                                \n",
        "            agent.step(states,actions,rewards,next_states,dones,beta)            \n",
        "            scores += rewards\n",
        "            states = next_states\n",
        "            if np.any(dones):\n",
        "                break              \n",
        "        max_score = np.max(scores)        \n",
        "        scores_deque.append(max_score)\n",
        "        all_scores.append(max_score)   \n",
        "        rolling_avg_100.append(np.mean(scores_deque))\n",
        "        print('\\rEpisode {} Episode Score:{:.2f} \\tAverage Score: {:.3f}'.format(i_episode, max_score, np.mean(scores_deque)), end=\"\")        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('\\rEpisode {}\\t  Episode Score:{:.2f} \\t Average Score: {:.3f}'.format(i_episode, max_score,np.mean(scores_deque)))\n",
        "                           \n",
        "        if np.mean(scores_deque)>=0.5:                    \n",
        "            agent.save_checkpt()\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode-100, np.mean(scores_deque)))            \n",
        "            break\n",
        "         \n",
        "    env.close()\n",
        "    return all_scores,rolling_avg_100\n",
        "scores,rolling_mean = maddpg()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyl6UN7t-OtT"
      },
      "source": [
        "### Plot Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahBEkIih8gUc"
      },
      "outputs": [],
      "source": [
        "def plot_result(scores,mean_scores,title_name):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.plot(np.arange(len(mean_scores)), mean_scores)\n",
        "    plt.title(title_name)\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.xlabel('Episode #')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzo7O7qk8gX2"
      },
      "outputs": [],
      "source": [
        "plot_result(list(range(100)),rewards_episode,\"Rewards Per Episode\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k7saYBa8okk"
      },
      "outputs": [],
      "source": [
        "plot_result(list(range(100)),policy_loss,\"Policy Loss Per Episode\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snsEDTXS8s5d"
      },
      "outputs": [],
      "source": [
        "plot_result(list(range(100)),value_loss,\"Value Loss per Episode\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGBxGQj_xGr"
      },
      "source": [
        "### *Reference*\n",
        "\n",
        "- https://github.com/Zorrorulz/MultiAgentDDPG-Tennis\n",
        "- https://github.com/starry-sky6688/MADDPG\n",
        "- https://github.com/shariqiqbal2810/MAAC\n",
        "- https://github.com/openai/maddpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWQMBrUF_yVZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
